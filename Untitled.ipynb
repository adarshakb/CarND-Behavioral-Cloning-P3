{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading csv...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../CarND-Behavioral-Cloning-Data/data_given/udacity/driving_log.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f5005a0d650d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-f5005a0d650d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0mtrainData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDATASETS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0mtrainData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreadCSV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0museStereo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepStraight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"total samples: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-f5005a0d650d>\u001b[0m in \u001b[0;36mreadCSV\u001b[0;34m(dataSet, keep, keepStraight, useStereo, flip)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mcsvFileName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'%sdriving_log.csv'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mcsvData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvFileName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipinitialspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../CarND-Behavioral-Cloning-Data/data_given/udacity/driving_log.csv'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "from PIL import Image\n",
    "from PIL import ImageOps\n",
    "#import argparse\n",
    "#import json\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Lambda, ELU\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras import callbacks\n",
    "import tensorflow as tf\n",
    "\n",
    "IMG_RES = 64\n",
    "IMG_X = IMG_RES\n",
    "IMG_Y = IMG_RES\n",
    "BATCHSIZE = 64\n",
    "DATASETS = ['udacity', 'track1_smooth', 'track1_recoverLeft', 'track1_recoverRight']\n",
    "EPOCHS = 10\n",
    "MODEL_PATH = 'F:/GitHub/CarND-Behavioral-Cloning-P3/models/'\n",
    "#MODEL_PATH = 'C:/Users/Marcus/Documents/GitHub/CarND-Behavioral-Cloning-P3/models/'\n",
    "DEBUG = False\n",
    "\n",
    "def readCSV(dataSet, keep=1.0, keepStraight=0.5, useStereo=False, flip=False):\n",
    "    '''\n",
    "    Reads data from drive log CSV and generates array of training data {img path, steering angle, img needs flipping}\n",
    "    '''\n",
    "    stereoSteerOffset = 0.25\n",
    "    dataDir = '../CarND-Behavioral-Cloning-Data/data_given/'\n",
    "    path = '%s%s/' % (dataDir, dataSet)\n",
    "    # read in csv\n",
    "    # centerImg, leftImg, rightImg, steering angle, throttle, break, speed\n",
    "    csvFileName = '%sdriving_log.csv' % (path)\n",
    "    csvData = []\n",
    "    with open(csvFileName, 'rt') as csvfile:\n",
    "        reader = csv.reader(csvfile, skipinitialspace=True)\n",
    "        for row in reader:\n",
    "            csvData.append( {'center': '%s%s' % (path, row[0]), 'left': '%s%s' % (path, row[1]), 'right': '%s%s' % (path, row[2]), 'steer': float(row[3])})\n",
    "    data = []\n",
    "    for line in csvData:\n",
    "        randKeep = np.random.uniform(0,1)\n",
    "        if randKeep < keep:\n",
    "            randKeepStraight = np.random.uniform(0,1)\n",
    "            if (abs(line['steer']) >= 0.05) or ((abs(line['steer']) < 0.05) and (randKeepStraight < keepStraight)):\n",
    "                data.append({'img': line['center'], 'steer': line['steer'], 'flip': False})\n",
    "                if flip:\n",
    "                    data.append({'img': line['center'], 'steer':line['steer']*-1, 'flip': True})\n",
    "                if useStereo:\n",
    "                    data.append({'img': line['left'], 'steer':line['steer']+stereoSteerOffset, 'flip': False})\n",
    "                    data.append({'img': line['right'], 'steer':line['steer']-stereoSteerOffset, 'flip': False})\n",
    "                    if flip:\n",
    "                        data.append({'img': line['left'], 'steer':(line['steer']+stereoSteerOffset)*-1,'flip': True})\n",
    "                        data.append({'img': line['right'], 'steer':(line['steer']-stereoSteerOffset)*-1 ,'flip': True})\n",
    "    return data\n",
    "\n",
    "\n",
    "def addShadow(img):\n",
    "    brightness = 0.5\n",
    "    height, width = img.shape[:2]\n",
    "    shdwWidth = 20+width*np.random.uniform()\n",
    "    shdwHeight = 20+height*np.random.uniform()\n",
    "\n",
    "    center_x = width*np.random.uniform()\n",
    "    center_y = height*np.random.uniform()\n",
    "\n",
    "    left_x = center_x-(shdwWidth/2)\n",
    "    right_x = center_x+(shdwWidth/2)\n",
    "    top_y = center_y-(shdwHeight/2)\n",
    "    bot_y = center_y+(shdwHeight/2)\n",
    "    \n",
    "    # ugly bounding box test...\n",
    "    for x in range(len(img)):\n",
    "        for y in range(len(img[x])):\n",
    "            if (x < right_x) and (x > left_x) and (y > top_y) and (y < bot_y):\n",
    "                img[x][y] = img[x][y] * brightness\n",
    "    return img\n",
    "\n",
    "\n",
    "def randBrightness(img):\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "    randBright = min(0.25+np.random.uniform(), 1.0)\n",
    "    hsv[:,:,2] = hsv[:,:,2] * randBright\n",
    "    img = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n",
    "    return img\n",
    "\n",
    "\n",
    "def preprocessImg(img, cropYtop=35, cropYbot=25, targetResize=IMG_RES, blur=False, flip=False, addShdw=False, randBright=False):\n",
    "    # convert from PIL to cv2\n",
    "    img = np.array(img)\n",
    "    height, width = img.shape[:2]\n",
    "    # CROP\n",
    "    imgCrop = img[cropYtop:height-cropYbot, 0:width]\n",
    "    # RANDOMIZE BRIGHTNESS\n",
    "    if randBright:\n",
    "        imgCrop = randBrightness(imgCrop)\n",
    "    # ADD SHADOW\n",
    "    if addShdw:\n",
    "        imgCrop = addShadow(imgCrop)\n",
    "    # RESIZE and make square (that way I can crop however I want and not worry about my model not fitting anymore)\n",
    "    imgRes = cv2.resize(imgCrop, (targetResize, targetResize))\n",
    "    # BLUR\n",
    "    if blur:\n",
    "        imgProc = cv2.GaussianBlur(imgRes,(3,3),0)\n",
    "    else:\n",
    "        imgProc = imgRes\n",
    "    # FLIP\n",
    "    if flip:\n",
    "        imgProc = np.fliplr(imgProc)\n",
    "    return imgProc\n",
    "\n",
    "\n",
    "def generateBatchRandom(trainData):\n",
    "    batchImg = np.zeros((BATCHSIZE, IMG_Y, IMG_X, 3))\n",
    "    batchSteer = np.zeros(BATCHSIZE)\n",
    "    while 1:\n",
    "        for i in range(BATCHSIZE):\n",
    "            i_data = np.random.randint(len(trainData))\n",
    "            img = Image.open(trainData[i_data]['img'])\n",
    "            batchImg[i] = preprocessImg(img, flip=trainData[i_data]['flip'], addShdw=True, randBright=True)\n",
    "            batchSteer[i] = trainData[i_data]['steer']\n",
    "        yield batchImg, batchSteer\n",
    "\n",
    "\n",
    "def generateBatch(trainData):\n",
    "    batchImg = np.zeros((BATCHSIZE, IMG_Y, IMG_X, 3))\n",
    "    batchSteer = np.zeros(BATCHSIZE)\n",
    "    while 1:\n",
    "        totalCount = 1\n",
    "        batchIndex = 0\n",
    "        for data in trainData:\n",
    "            img = Image.open(data['img'])                   \n",
    "            batchImg[batchIndex] = preprocessImg(img, flip=data['flip'])\n",
    "            batchSteer[batchIndex] = data['steer']\n",
    "            if (batchIndex == BATCHSIZE-1) or (totalCount == len(trainData)):\n",
    "                if DEBUG:\n",
    "                    print('\\nyielding batch up to %s with batch size %s' % (totalCount, batchIndex+1))\n",
    "                # cut down batch array if it is smaller than BATCHSIZE (otherwise we pass zero-values to the trainer)\n",
    "                if batchIndex < BATCHSIZE-1:\n",
    "                    batchImg = batchImg[:batchIndex+1]\n",
    "                    batchSteer = batchSteer[:batchIndex+1]\n",
    "                yield batchImg, batchSteer\n",
    "                # reset everything\n",
    "                batchImg = np.zeros((BATCHSIZE, IMG_Y, IMG_X, 3))\n",
    "                batchSteer = np.zeros(BATCHSIZE)\n",
    "                batchIndex = -1\n",
    "            batchIndex += 1\n",
    "            totalCount += 1\n",
    "\n",
    "\n",
    "def nVidia(ch, row, col):\n",
    "    #ch, row, col = 3, 160, 320  # original model format\n",
    "    model = Sequential()\n",
    "    model.add(Lambda(lambda x: x/127.5 - 1.,\n",
    "              input_shape=(row, col, ch),\n",
    "              output_shape=(row, col, ch)))\n",
    "    #model.add(Convolution2D(24, 5, 5, subsample=(2, 2), border_mode=\"same\"))\n",
    "    model.add(Convolution2D(24, 3, 3, subsample=(2, 2), border_mode=\"same\"))\n",
    "    model.add(ELU())\n",
    "    #model.add(Convolution2D(36, 5, 5, subsample=(2, 2), border_mode=\"same\"))\n",
    "    model.add(Convolution2D(36, 3, 3, subsample=(2, 2), border_mode=\"same\"))\n",
    "    model.add(ELU())\n",
    "    #model.add(Convolution2D(48, 5, 5, subsample=(2, 2), border_mode=\"same\"))\n",
    "    model.add(Convolution2D(48, 3, 3, subsample=(2, 2), border_mode=\"same\"))\n",
    "    model.add(ELU())\n",
    "    model.add(Convolution2D(64, 3, 3, subsample=(1, 1), border_mode=\"same\"))\n",
    "    model.add(ELU())\n",
    "    model.add(Convolution2D(64, 3, 3, subsample=(1, 1), border_mode=\"same\"))\n",
    "    model.add(ELU())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1164))\n",
    "    model.add(ELU())\n",
    "    model.add(Dense(100))\n",
    "    model.add(ELU())\n",
    "    model.add(Dense(50))\n",
    "    model.add(ELU())\n",
    "    model.add(Dense(10))\n",
    "    model.add(ELU())\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "def commaAI(ch, row, col):\n",
    "    #ch, row, col = 3, 160, 320  # original model format\n",
    "    with tf.name_scope('model'):\n",
    "        model = Sequential()\n",
    "        with tf.name_scope('normalize'):\n",
    "            model.add(Lambda(lambda x: x/127.5 - 1.,\n",
    "                      input_shape=(row, col, ch),\n",
    "                      output_shape=(row, col, ch)))\n",
    "        with tf.name_scope('conv2D_1'):\n",
    "            #model.add(Convolution2D(16, 8, 8, subsample=(4, 4), border_mode=\"same\"))\n",
    "            model.add(Convolution2D(16, 5, 5, subsample=(4, 4), border_mode=\"same\"))\n",
    "        with tf.name_scope('ELU_1'):\n",
    "            model.add(ELU())\n",
    "        with tf.name_scope('conv2D_2'):\n",
    "            #model.add(Convolution2D(32, 5, 5, subsample=(2, 2), border_mode=\"same\"))\n",
    "            model.add(Convolution2D(32, 3, 3, subsample=(2, 2), border_mode=\"same\"))\n",
    "        with tf.name_scope('ELU_2'):\n",
    "            model.add(ELU())\n",
    "        with tf.name_scope('conv2D_3'):\n",
    "            model.add(Convolution2D(64, 5, 5, subsample=(2, 2), border_mode=\"same\"))\n",
    "            model.add(Flatten())\n",
    "        with tf.name_scope('dropout_1'):\n",
    "            model.add(Dropout(.2))\n",
    "        with tf.name_scope('ELU_3'):\n",
    "            model.add(ELU())\n",
    "        with tf.name_scope('dense_1'):\n",
    "            model.add(Dense(512))\n",
    "        with tf.name_scope('dropout_2'):\n",
    "            model.add(Dropout(.5))\n",
    "        with tf.name_scope('elu_4'):\n",
    "            model.add(ELU())\n",
    "        with tf.name_scope('dense_2'):\n",
    "            model.add(Dense(1))\n",
    "        with tf.name_scope('learning'):\n",
    "            model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "def dummyModel(ch, row, col):\n",
    "    model = Sequential()\n",
    "    model.add(Lambda(lambda x: x/127.5 - 1.,\n",
    "              input_shape=(row, col, ch),\n",
    "              output_shape=(row, col, ch)))\n",
    "    model.add(Convolution2D(16, 3, 3, subsample=(2, 2), border_mode=\"same\"))\n",
    "    model.add(ELU())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(ELU())\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"reading csv...\")\n",
    "    trainData = []\n",
    "    for dataset in DATASETS:\n",
    "        trainData.extend(readCSV(dataset, useStereo=True, flip=True, keepStraight=0.5, keep = 1.0))\n",
    "    print(\"total samples: %s\" % len(trainData))\n",
    "\n",
    "    print(\"building model...\")\n",
    "    #model = nVidia(3, IMG_Y, IMG_X)\n",
    "    model = commaAI(3, IMG_Y, IMG_X)\n",
    "    cbks = [callbacks.TensorBoard(log_dir='tb_log/')]\n",
    "\n",
    "    print(\"starting training...\")\n",
    "    generator = generateBatchRandom(trainData)\n",
    "    with tf.name_scope('train'):\n",
    "        model.fit_generator(generator, callbacks=cbks, samples_per_epoch=20000, nb_epoch=EPOCHS)\n",
    "\n",
    "    print(\"saving model...\")\n",
    "    modelName = 'commaAI_bigData_20k_%se' % EPOCHS\n",
    "    fileName = '%s%s' % (MODEL_PATH, modelName)\n",
    "    json_string = model.to_json()\n",
    "    with open('%s.json' % fileName, \"w\") as json_file:\n",
    "        json_file.write(json_string)\n",
    "    model.save_weights('%s.h5' % fileName)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:carnd-term1]",
   "language": "python",
   "name": "conda-env-carnd-term1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
